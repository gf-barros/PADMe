{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - PADMe - Basic Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything, make sure that the conda environment is installed. Using the terminal, once you are at the root directory of this project, just type:\n",
    "\n",
    "`make create_environment`\n",
    "\n",
    "And then:\n",
    "\n",
    "`make requirements`\n",
    "\n",
    "Finally, to convert our conda env to Jupyter kernel, type:\n",
    "\n",
    "`make env_to_kernel`\n",
    "\n",
    "If no errors were found, we are good to go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the very first Jupyter Notebook using PADMe for Dynamic Mode Decomposition processing. We start by importing the necesssary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path \n",
    "\n",
    "from natsort import natsorted # Required for sorting filename correctly\n",
    "from dotenv import find_dotenv # Required for locating project's root dir\n",
    "\n",
    "from src import DMD # DMD class for ingesting and processing data\n",
    "from src import snapshots_assembly # Function for assembling the Snapshots Matrix\n",
    "from src import (\n",
    "    load_notebook_params, # Function that loads predefined examples and paths\n",
    "    generate_paths, # Function that creates the paths to datasets\n",
    "    download_dataset, # Function that downloads the dataset from Kaggle\n",
    "    unpack_kaggle_dataset, # Function that unzips the downloaded dataset\n",
    ")\n",
    "\n",
    "from src import PostProcessingDMD # Class used for postprocessing and dataviz\n",
    "\n",
    "root_dir = os.path.dirname(find_dotenv())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the `root_dir` is correctly set up. This is required for properly pointing directions to the locations where data will be stored. Make sure `root_dir` points to the root directory of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get all notebook params. This function reads from a `.yaml` file in the `notebooks/` directory that contains example file names and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_params = load_notebook_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before downloading dataset, you need to create an account on Kaggle. Instructions for using the Kaggle API can be seen at sections **Installation** and **Authentication** on https://www.kaggle.com/docs/api. \n",
    "\n",
    "Basically, you need to:\n",
    "\n",
    "1. Create a Kaggle account at https://www.kaggle.com/.\n",
    "2. Click on you avatar (top right) and then `settings`.\n",
    "3. At the API section, click `Create New Token`. This will generate your `kaggle.json` file and make it available for download.\n",
    "4. Create a hidden directory on your home path e.g. `mkdir $HOME/.kaggle` or `cd && mkdir .kaggle`.\n",
    "5. Copy and paste your `kaggle.json` file into the `.kaggle` directory.\n",
    "\n",
    "Once your `~/.kaggle/kaggle.json` file is set up, we can download the datasets used in this example.\n",
    "\n",
    "For this notebook, we will used the `covid-19-spread-in-lombardy-italy` dataset. To download this dataset, the `example_dataset` variable can be either `sird` or `seird`. This dataset was generated when modeled the COVID-19 spread in the Lombardy region (Italy) using PDE-based compartmental models spatially discretized using the Finite Element Method. \n",
    "  \n",
    "This zip file contains 991MB and contains two versions of the simulation. \n",
    "The datasets are:\n",
    "- `covid-sird-lombardy-freefem` where a SIRD model was approximated and modeled using the FreeFEM++ library. More details can be seen on: `citar artigo do Alex aqui`\n",
    "- `covid-seird-lombardy-libmesh` where a SEIRD model was approximated and modeled using the libMesh library. More details can be seen on: `citar artigo da Malu aqui`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = \"sird\"\n",
    "\n",
    "dict_paths = generate_paths(example_dataset, root_dir, notebook_params)\n",
    "print(dict_paths)\n",
    "\n",
    "download_dataset(dict_paths, example_dataset, notebook_params, FORCE_DOWNLOAD=False)\n",
    "unpack_kaggle_dataset(dict_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell was correctly executed, there is a zipfile in your `padme/data/00_raw/` folder and two directories, containing each one of the datasets described. They are in their raw form, that is, exactly in the form where the numerical simulations output them.\n",
    "\n",
    "This is one of the most annoying parts of DMD architectures and **PADMe** aims to automatize this procedure. If you explore the two generated folders, you will see that the `susceptible` snapshots on the `covid-sird-lombardy-freefem` dataset are of the form:\n",
    "- `covid-sird-lombardy-freefem/covid-sird-lombardy-freefem/4susceptible.vtk`\n",
    "- `covid-sird-lombardy-freefem/covid-sird-lombardy-freefem/8susceptible.vtk`\n",
    "- `covid-sird-lombardy-freefem/covid-sird-lombardy-freefem/12susceptible.vtk`\n",
    "\n",
    "while the `susceptible` snapshots on the `covid-seird-lombardy-libmesh` dataset are of the form:\n",
    "- `covid-seird-lombardy-libmesh/covid-seird-lombardy-libmesh/step0/out_1_000_00000.h5`\n",
    "- `covid-seird-lombardy-libmesh/covid-seird-lombardy-libmesh/step0/out_1_000_00001.h5`\n",
    "- `covid-seird-lombardy-libmesh/covid-seird-lombardy-libmesh/step0/out_1_000_00002.h5`\n",
    "being `s` the dataset inside the `.h5` files containing the susceptibles data.\n",
    "\n",
    "This difference in structure delays the application of DMD to multiple datasets and usually requires attention and time of the engineer. Using **PADMe**, we need to create a list of strings (or Path) containing the snapshots absolute filename. The following cell does that for the first case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_walk_files = next(os.walk(dict_paths[\"snapshots_filepath\"]))\n",
    "files = natsorted(os_walk_files[2])\n",
    "\n",
    "filenames = [\n",
    "    dict_paths[\"snapshots_filepath\"] / Path(f\"{str(i)}infected.vtk\")\n",
    "    for i, _ in zip(range(4, 480, 4), files)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the five first and last filenames are correct: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[-5:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our list of filenames is complete, we can create the `snapshot_ingestion_parameters` dictionary. Given that the `sird` simulation consists of ASCII `.vtk` files, we can take a look at one of them to see the starting and ending lines where nodal values were stored. In this case, the dictionary is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_ingestion_parameters = {\n",
    "    \"filenames\": filenames,\n",
    "    \"starting_line\": 125939,\n",
    "    \"ending_line\": 210239,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our dataset using `snapshots_assembly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = snapshots_assembly(\n",
    "    \"vtk_freefem\", snapshot_ingestion_parameters=snapshot_ingestion_parameters\n",
    ")\n",
    "dataset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our snapshots matrix contain all snapshots and dimensions of `(84300, 119)`. It is now ready to be ingested on the DMD class. Let's see how it works for libMesh/EdgeCFD `.h5` files. We need to regenerate the `dict_paths` that points to the folders containing the data. We don't need to redownload any datasets, but the functions will check that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = \"seird\"\n",
    "\n",
    "dict_paths = generate_paths(example_dataset, root_dir, notebook_params)\n",
    "dict_paths\n",
    "\n",
    "download_dataset(dict_paths, example_dataset, notebook_params, FORCE_DOWNLOAD=False)\n",
    "unpack_kaggle_dataset(dict_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now proceeding with our list of filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_walk_files = next(os.walk(dict_paths[\"snapshots_filepath\"]))\n",
    "folders = natsorted(os_walk_files[1])\n",
    "\n",
    "filenames = [\n",
    "    dict_paths[\"snapshots_filepath\"]\n",
    "    / Path(snapshot_folder)\n",
    "    / Path(f\"out_1_000_{str(i).zfill(5)}.h5\")\n",
    "    for i, snapshot_folder in enumerate(folders)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the beginning and the end of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[-5:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fill the `snapshot_ingestion_dictionary` and let's assemble our snapshots matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_ingestion_parameters = {\n",
    "    \"filenames\": filenames,\n",
    "    \"dataset\": \"s\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = snapshots_assembly(\n",
    "    \"h5_libmesh\", snapshot_ingestion_parameters=snapshot_ingestion_parameters\n",
    ")\n",
    "dataset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have our snapshots matrix containing dimensions `(13158, 482)`. Let's fill the `dmd_parameters` dictionary and process this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd_parameters = {\n",
    "    \"factorization_algorithm\": \"randomized_svd\",\n",
    "    \"basis_vectors\": 25,\n",
    "    \"randomized_svd_parameters\": {\n",
    "        \"power_iterations\": 1,\n",
    "        \"oversampling\": 20,\n",
    "    },\n",
    "    \"starting_step\": 25,  # Can be 0 for susceptibles, but should be >0 for zero initialized fields.\n",
    "    \"dt_simulation\": 0.05,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate our `DMD` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd = DMD(dataset, dmd_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.factorization()` method is responsible for calling the factorization method added in the `dmd_parameters` dictionary. In this case, it is the `randomized_svd` algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd.factorization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the outputs obtained on objects instantiated on the `DMD` class contain an attribute called `.dmd_approximation`. This attribute is a dictionary containing all possible outputs from the DMD processing. Let's see our singular values from the `.factorization()` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd.dmd_approximation[\"s\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's proceed to compute our DMD approximation. We invoke the `.dmd_core()` method from the `dmd` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd.dmd_core()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd.dmd_approximation.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.dmd_approximation` attribute contains the following keys:\n",
    "- `'snapshots_matrix'` : the sliced snapshots_matrix `[starting_step : ending_step]`\n",
    "- `'u'` : the $\\mathbf{U}$ matrix from SVD\n",
    "- `'s'` : the $\\mathbf{\\Sigma}$ matrix from SVD\n",
    "- `'vt'` : the $\\mathbf{V}^T$ matrix from SVD\n",
    "- `'eigenvals_original'` : Eigenvalues from $\\tilde{\\mathbf{A}}$ matrix\n",
    "- `'eigenvals_processed'` : Eigenvalues from $\\tilde{\\mathbf{A}}$ matrix after $log(\\Lambda)/\\Delta t$\n",
    "- `'t'` : array containing time steps used for approximation\n",
    "- `'dmd_matrix'` : the DMD approximation for all times existent in key `'t'`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can postprocess our data and visualize the results. Let's instantiate the `PostProcessingDMD` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd_visualizer = PostProcessingDMD(dmd.dmd_approximation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the singular values using the `.plot_singular_values()` method. All visualization methods are implemented for `matplotlib`, `seaborn` and `plotly` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd_visualizer.plot_singular_values(\"seaborn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does make sense. Now let's take a look at the eigenvalues of $\\mathbf{\\tilde{A}}$ and see if our modes are stable. We can plot the eigenvalues using the `.plot_eigenvalues()` method`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd_visualizer.plot_eigenvalues(\"plotly\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see the quality of our approximation. We can compute the $\\mathcal{L}_2$ norm in time between approximation and original data with the `.compute_temporal_l2_norm()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd_visualizer.compute_temporal_l2_norm(\"seaborn\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2882c81422a800f9d7f60556fa066fc0df92ff6015fbd6757aaa84163cbdb3a5"
  },
  "kernelspec": {
   "display_name": "Python 3.11.2 ('padme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
